{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Definition\n",
    "Variable\tDefinition\tKey\n",
    "survival\tSurvival\t0 = No, 1 = Yes\n",
    "pclass\t\tTicket class\t1 = 1st = Upper, 2 = 2nd = Middle, 3 = 3rd = Lower\n",
    "sex\t\t\tSex\t\n",
    "Age\t\t\tAge in years (fractional if less than 1. If the age is estimated, is it in the form of xx.5)\n",
    "sibsp\t\t# of siblings / spouses aboard the Titanic\n",
    "\tSibling = brother, sister, stepbrother, stepsister\n",
    "\tSpouse = husband, wife (mistresses and fianc√©s were ignored)\n",
    "parch\t\t# of parents / children aboard the Titanic\t\n",
    "\tParent = mother, father\n",
    "\tChild = daughter, son, stepdaughter, stepson\n",
    "\tSome children travelled only with a nanny, therefore parch=0 for them.\n",
    "ticket\t\tTicket number\t\n",
    "fare\t\tPassenger fare\t\n",
    "cabin\t\tCabin number\t\n",
    "embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load titanic data frame, create new columns 'dataset' then concatenate the 2 data\n",
    "titanic_train = pd.read_csv('train.csv')\n",
    "titanic_test = pd.read_csv('test.csv')\n",
    "\n",
    "titanic_train['dataset'] = 'train'\n",
    "titanic_test['dataset'] = 'test'\n",
    "\n",
    "df = [titanic_train, titanic_test]\n",
    "titanic = pd.concat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def change_col(data):\n",
    "    new_col = [i.lower().replace(' ','_').replace('#','no') for i in data.columns]\n",
    "    data.columns = new_col\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'cabin', 'embarked', 'fare', 'name', 'parch', 'passengerid',\n",
       "       'pclass', 'sex', 'sibsp', 'survived', 'ticket', 'dataset'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = change_col(titanic)\n",
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new columns based on names (LastName, FirstName and Title)\n",
    "titanic['name_split'] = titanic.name.str.split('\\, ')\n",
    "titanic['lname'] = titanic.name_split.str.get(0)\n",
    "titanic['fname'] = titanic.name_split.str.get(1)\n",
    "titanic['fname'] = titanic.fname.str.split('\\. ')\n",
    "titanic['title'] = titanic.fname.str.get(0)\n",
    "titanic['fname'] = titanic.fname.str.get(1)\n",
    "del titanic['name_split']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NULL fare since info above stated that there is 1 missing fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping NULL fares since it will not affect our analysis\n",
    "titanic = titanic.loc[titanic.fare.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many values that are missing for Cabin, let us forget about it for now and focus on other columns. Let us create a Gender column separating the Male, Female and Child passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def male_female_child(passenger):\n",
    "    age, sex = passenger\n",
    "    if age < 14:\n",
    "        return 'child'\n",
    "    else:\n",
    "        return sex    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic['gender'] = titanic[['age','sex']].apply(male_female_child,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting value for embarked, checking as there are 2 NULLs found.\n",
    "tfare = titanic.loc[titanic.embarked.isnull(),'fare'].mean()\n",
    "\n",
    "# Setting value to C as found in above results\n",
    "titanic.loc[titanic.embarked.isnull(),'embarked'] = 'C'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new column for those people with Family and those that travelled alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic['w_family'] = titanic.parch + titanic.sibsp\n",
    "titanic.loc[titanic.w_family > 0,'w_family'] = 1\n",
    "titanic.loc[titanic.w_family == 0,'w_family'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic['cabin_letter'] = titanic.cabin.str[0]\n",
    "titanic['cabin_number'] = titanic.cabin.str[-1]\n",
    "\n",
    "def cabin_side(number):\n",
    "    if number in ('1','3','5','7','9'):\n",
    "        return 'right'\n",
    "    elif number in ('2','4','6','8','0'):\n",
    "        return 'left'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "titanic['cabin_side'] = titanic['cabin_number'].apply(cabin_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_new = titanic.title.copy()\n",
    "\n",
    "#Change Ms and Mlle to Miss\n",
    "filt = ['Mr','Mrs','Miss','Master']\n",
    "title_new[title_new.isin(['Ms','Mlle'])] = 'Miss' \n",
    "title_new[title_new == 'Mme'] = 'Mrs'\n",
    "title_new[~title_new.isin(filt)] = 'Honorifics'\n",
    "\n",
    "titanic.loc[:,'new_title'] = title_new\n",
    "titanic.loc[np.logical_and(titanic.new_title == 'Honorifics',\n",
    "                                 titanic.sex == 'male'),'new_title'] = 'Honorific_male'\n",
    "titanic.loc[np.logical_and(titanic.new_title == 'Honorifics',\n",
    "                                 titanic.sex == 'female'),'new_title'] = 'Honorific_female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Separate Actual Data with Holdout Data\n",
    "titanic_train = titanic.loc[titanic['dataset'] == 'train']\n",
    "titanic_test = titanic.loc[titanic['dataset'] == 'test']\n",
    "\n",
    "# removed changed columns\n",
    "data = titanic_train\n",
    "data = data.drop(['name','parch','sibsp','sex','title','cabin'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing to make the Prediction Model\n",
    "data = data.loc[data.age.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define combine_text_columns()\n",
    "NUMERIC_COLUMNS = ['age','fare','survived','passengerid','pclass','dataset','w_family','fname','cabin_letter','cabin_side']\n",
    "\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis = 1)\n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace = True)\n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Modifying columns and Preprocessings\n",
    "change_cols = FunctionTransformer(change_col, validate=False)\n",
    "\n",
    "\n",
    "# Preprocessing before Classifier\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x.loc[:,['age','fare','w_family','pclass']], validate=False)\n",
    "get_cabin_data = FunctionTransformer(lambda x: pd.get_dummies(x[['cabin_letter','cabin_side']]), validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 331 entries, 0 to 415\n",
      "Data columns (total 22 columns):\n",
      "age             331 non-null float64\n",
      "cabin           87 non-null object\n",
      "embarked        331 non-null object\n",
      "fare            331 non-null float64\n",
      "name            331 non-null object\n",
      "parch           331 non-null int64\n",
      "passengerid     331 non-null int64\n",
      "pclass          331 non-null int64\n",
      "sex             331 non-null object\n",
      "sibsp           331 non-null int64\n",
      "survived        0 non-null float64\n",
      "ticket          331 non-null object\n",
      "dataset         331 non-null object\n",
      "lname           331 non-null object\n",
      "fname           331 non-null object\n",
      "title           331 non-null object\n",
      "gender          331 non-null object\n",
      "w_family        331 non-null int64\n",
      "cabin_letter    87 non-null object\n",
      "cabin_number    87 non-null object\n",
      "cabin_side      331 non-null object\n",
      "new_title       331 non-null object\n",
      "dtypes: float64(3), int64(5), object(14)\n",
      "memory usage: 59.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data_test = titanic_test.loc[titanic_test.age.notnull()]\n",
    "\n",
    "data_test.info()\n",
    "\n",
    "data_test = data_test.drop(['parch','sibsp','sex','title','cabin'], axis = 1)\n",
    "data_test = data_test.sort_values(by='name')\n",
    "data_test = data_test.reset_index()\n",
    "\n",
    "# loading survival data acquired online, this will be used to check the holdout data\n",
    "titanicOnline = pd.read_csv('test_set.csv')\n",
    "titanicOnline = titanicOnline.loc[np.logical_and(titanicOnline.Age.notnull(),titanicOnline.Fare.notnull()), ['PassengerID', 'Pclass','Name','Sex','Age','Fare','Embarked','Survived']]\n",
    "\n",
    "titanic_val = titanicOnline.loc[:,['Name','Survived']].sort_values(by='Name')\n",
    "titanic_val = titanic_val.reset_index()\n",
    "\n",
    "data_test = data_test.sort_values(by='name')\n",
    "data_test = data_test.reset_index()\n",
    "\n",
    "validate = np.empty(len(data_test))\n",
    "for x in np.arange(len(data_test)):\n",
    "    data_test.loc[x,'survived'] = titanic_val.loc[x,'Survived']\n",
    "\n",
    "data_test = data_test.drop(['level_0','index','name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('survived',axis = 1), data.survived, test_size = 0.3, random_state=2)\n",
    "holdout_X = data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the actual prediction model for kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('numeric_features', Pipeline(steps=[('selector', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x000000000B12B400>, pass_y=False,\n",
       "          validate=False)), ('imputer', Imputer(axis=0, copy=True, missi...3, 44, 45, 46, 47, 48, 49])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using K-fold Cross validation\n",
    "param_grid = {'n_neighbors': np.arange(1,50)}\n",
    "\n",
    "# Using GridSearchCV to get the best Neighbor #\n",
    "knn = KNeighborsClassifier()\n",
    "pl1 = Pipeline([('union', FeatureUnion(transformer_list = [\n",
    "                ('numeric_features', Pipeline([('selector', get_numeric_data),\n",
    "                                            ('imputer', Imputer())])),\n",
    "                ('cabin', get_cabin_data),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC))\n",
    "                                           ]))\n",
    "                                    ])),    \n",
    "        ('clf', GridSearchCV(knn, param_grid, cv = 5))])\n",
    "\n",
    "pl1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl = Pipeline([('union', FeatureUnion(transformer_list = [\n",
    "                ('numeric_features', Pipeline([('selector', get_numeric_data),\n",
    "                                            ('imputer', Imputer())])),\n",
    "                ('cabin', get_cabin_data),\n",
    "                ('text_features', Pipeline([('selector', get_text_data),\n",
    "                                            ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC))\n",
    "                                           ]))\n",
    "                                    ])),    \n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "# pl1.named_steps['clf'].best_params_['n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('numeric_features', Pipeline(steps=[('selector', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x000000000B12B400>, pass_y=False,\n",
       "          validate=False)), ('imputer', Imputer(axis=0, copy=True, missi...=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = pl.predict(X_test)\n",
    "p_holdout = pl.predict(data_test.drop('survived',axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83255813953488367"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68277945619335345"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(data_test.drop('survived',axis = 1),data_test.loc[:,'survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test.loc[:,'survived_pred'] = p_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>lname</th>\n",
       "      <th>survived_pred</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eugene Joseph</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karen Marie</td>\n",
       "      <td>Abelseth</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olaus Jorgensen</td>\n",
       "      <td>Abelseth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abraham August Johannes</td>\n",
       "      <td>Abrahamsson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joseph (Sophie Halaut Easu)</td>\n",
       "      <td>Abrahim</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Philip Frank</td>\n",
       "      <td>Aks</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Charles Augustus</td>\n",
       "      <td>Aldworth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hudson Joshua Creighton</td>\n",
       "      <td>Allison</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Albert Karvin</td>\n",
       "      <td>Andersen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ida Augusta Margareta</td>\n",
       "      <td>Andersson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fname        lname  survived_pred  survived\n",
       "0                Eugene Joseph       Abbott            1.0       0.0\n",
       "1                  Karen Marie     Abelseth            1.0       1.0\n",
       "2              Olaus Jorgensen     Abelseth            0.0       1.0\n",
       "3      Abraham August Johannes  Abrahamsson            0.0       0.0\n",
       "4  Joseph (Sophie Halaut Easu)      Abrahim            1.0       0.0\n",
       "5                 Philip Frank          Aks            1.0       1.0\n",
       "6             Charles Augustus     Aldworth            0.0       0.0\n",
       "7      Hudson Joshua Creighton      Allison            0.0       0.0\n",
       "8                Albert Karvin     Andersen            0.0       0.0\n",
       "9        Ida Augusta Margareta    Andersson            0.0       0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.loc[:,['fname','lname','survived_pred','survived']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.loc[np.logical_and(data_test.survived_pred == 1.0,data_test.survived == 0.0),'fname'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.loc[np.logical_and(data_test.survived_pred == 0.0,data_test.survived == 1.0),'fname'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.loc[data_test.survived_pred == data_test.survived,'fname'].count()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
